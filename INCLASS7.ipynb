{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKSTLF2BX6jH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N11Ee3GJmywu",
        "outputId": "502361fd-9747-4237-931c-98b5c03d791b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=aab3fc83625369b67be4f6665431f927d89b1ae8e93d6a3ecba1da052b3da812\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: h11, wikipedia, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0 wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import wikipedia"
      ],
      "metadata": {
        "id": "Q2A8TGhKm3i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.) Set up OpenAI and the enviornment\n"
      ],
      "metadata": {
        "id": "7E9HEMJSX-3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DONE\n",
        "apikey = \"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\""
      ],
      "metadata": {
        "id": "4zwwdkZDYDZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = apikey"
      ],
      "metadata": {
        "id": "8IiKS0snlpYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(\n",
        "    api_key = apikey\n",
        ")"
      ],
      "metadata": {
        "id": "UbXtUHPkMsN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AqyqgUkRMsS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.) Use the wikipedia api to get a function that pulls in the text of a wikipedia page"
      ],
      "metadata": {
        "id": "tOXc5_BTm9HP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "page_titles = ['Artificial', 'Intelligence', \"UCLA\"]"
      ],
      "metadata": {
        "id": "-v7OYamHlrEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_title = page_titles[0]"
      ],
      "metadata": {
        "id": "TgY2FkTdmhTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_results = wikipedia.search(page_title)"
      ],
      "metadata": {
        "id": "aPGn--kPNYVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page = wikipedia.search(search_results[0])"
      ],
      "metadata": {
        "id": "mCME4AQqNdq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(wikipedia)"
      ],
      "metadata": {
        "id": "Kw5H5jMlmmS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df95e4e0-092e-479f-f29a-ddd0d3dc5554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['API_URL',\n",
              " 'BeautifulSoup',\n",
              " 'Decimal',\n",
              " 'DisambiguationError',\n",
              " 'HTTPTimeoutError',\n",
              " 'ODD_ERROR_MESSAGE',\n",
              " 'PageError',\n",
              " 'RATE_LIMIT',\n",
              " 'RATE_LIMIT_LAST_CALL',\n",
              " 'RATE_LIMIT_MIN_WAIT',\n",
              " 'RedirectError',\n",
              " 'USER_AGENT',\n",
              " 'WikipediaException',\n",
              " 'WikipediaPage',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '__version__',\n",
              " 'cache',\n",
              " 'datetime',\n",
              " 'debug',\n",
              " 'donate',\n",
              " 'exceptions',\n",
              " 'geosearch',\n",
              " 'languages',\n",
              " 'page',\n",
              " 'random',\n",
              " 're',\n",
              " 'requests',\n",
              " 'search',\n",
              " 'set_lang',\n",
              " 'set_rate_limiting',\n",
              " 'set_user_agent',\n",
              " 'stdout_encode',\n",
              " 'suggest',\n",
              " 'summary',\n",
              " 'sys',\n",
              " 'time',\n",
              " 'timedelta',\n",
              " 'unicode_literals',\n",
              " 'util',\n",
              " 'wikipedia']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZF3BiZyXltYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wikipedia_content(page_title):\n",
        "  search_results = wikipedia.search(page_title)\n",
        "  page = wikipedia.page(search_results[0])\n",
        "\n",
        "  return (page.content)"
      ],
      "metadata": {
        "id": "Ef7yfa2jl0iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = get_wikipedia_content(page_title)"
      ],
      "metadata": {
        "id": "R4BUh4mvN_rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jGkJSleTN_v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.) Build a chatgpt bot that will analyze the text given and try to locate any false info"
      ],
      "metadata": {
        "id": "_9aruncMmubX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completions = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a summary assistant at Wikipedia, I will pass you an article and please tell me if any of the information is false\"},\n",
        "        {\"role\": \"user\", \"content\": content[:8180]}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Bmai3B6Dmw3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completions.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU3xM6ZXr8Yb",
        "outputId": "9604ca8a-6768-42b0-c894-7135269754c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The information provided in the article is accurate. Artificiality refers to the state of being a product of intentional human manufacturing, instead of being naturally occurring. It can have the implication of falsehood or deception or illustrate the ability of humans to replicate naturally occurring forms or functions. The distinction between natural and artificial objects is generally possible because artificial environments often have more spatial and temporal physical regularity than natural ones. Therefore, there are no false facts in the article.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt_error_correction(test):\n",
        "    chat_completions = client.chat.completions.create(\n",
        "    model = 'gpt-4',\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': 'I will be giving you an article. I am looking for false information. I want to capture all potentially false info, if there is even small potential for it to be wrong, please return it. PleaseIf there is no false information only return Done!'},\n",
        "        {'role': 'user', 'content': content[:8180]}]\n",
        "    )\n",
        "    print(chat_completions.choices[0].message.content)"
      ],
      "metadata": {
        "id": "4O-SWvEAXeDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1jI-un5PnDjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TMKFGN4nDJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FKAJVXSoayA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.) Make a for loop and check a few wikipedia pages and return a report of any potentially false info via wikipedia"
      ],
      "metadata": {
        "id": "zPw5LyPEobmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_titles = ['Artificial', 'Intelligence', 'UCLA']"
      ],
      "metadata": {
        "id": "V7cuhML2ocGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for page_title in page_titles:\n",
        "  try:\n",
        "    print('________________' + page_title)\n",
        "    content = get_wikipedia_content(page_title)\n",
        "    chatgpt_error_correction(content)\n",
        "  except:\n",
        "    print('ERROR')"
      ],
      "metadata": {
        "id": "uxu7tKzwRdYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04e96a7-1ce7-4c6a-c7dc-172e9b91b4f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "________________Artificial\n",
            "1. \"Artificiality (the state of being artificial or anthropogenic) is the state of being the product of intentional human manufacture, rather than occurring naturally through processes not involving or requiring human activity.\" - Without a clear context, the term 'artificiality' may have different interpretations.\n",
            "2. \"The philosopher Aristotle wrote in his Rhetoric...\" - This lacks actual quotes or translation from Aristotle's Rhetoric to verify its authenticity.\n",
            "3. \"Political scientist and artificial intelligence expert Herbert A. Simon observes that...\" - There's no direct quote or source mentioned for Simon's views.\n",
            "4. \"Some philosophers have gone further and asserted that, in a deterministic world, 'everything is natural and nothing is artificial', because everything in the world (including everything made by humans) is a product of the physical laws of the world.\" - This statement does not specifically mention which philosophers have asserted this.\n",
            "5. \"It is generally possible for humans, and in some instances, for computers, to distinguish natural from artificial environments.\" - This claim lacks supporting evidence or context.\n",
            "6. \"For example, by identifying and imitating natural means of pattern formation, some types of automata have been used to generate organic-looking textures for more realistic shading of 3D objects.\" - The specific types of automata are not mentioned. The claim lacks reference or any sort of evidence.\n",
            "7. The \"See also\" section lists terms without context or explanation of how they link to the topic of artificiality.\n",
            "________________Intelligence\n",
            "1. \"The term rose to prominence during the early 1900s. Most psychologists believe that intelligence can be divided into various domains or competencies.\" - The timeline and majority belief are difficult to confirm and could potentially be false.\n",
            "2. \"Intelligence is most often studied in humans but has also been observed in both non-human animals and plants despite controversy as to whether some of these forms of life exhibit intelligence.\" - The controversy suggests there is potential for false information. \n",
            "3. \"Intelligence in computers or other machines is called artificial intelligence.\" - This oversimplifies the definition of artificial intelligence and could be misleading. \n",
            "4. \"The definition of intelligence is controversial, varying in what its abilities are and whether or not it is quantifiable.\" - This statement presents potential for misinterpretation or controversy.\n",
            "5. \"It was signed by fifty-two researchers, out of 131 total invited to sign, with 48 explicitly refusing to sign.\" - The exact numbers provided here could be inaccurate. \n",
            "6. \"From Intelligence: Knowns and Unknowns (1995), a report published by the Board of Scientific Affairs of the American Psychological Association, also in response to controversy over The Bell Curve:\" - The exact dates, names and context could potentially be false. \n",
            "7. \"Intelligence enables humans to remember descriptions of things and use those descriptions in future behaviors.\" - This is a potential oversimplification, granting the opportunity for false information.\n",
            "8. \"There have been various attempts to quantify intelligence via testing, such as the Intelligence Quotient (IQ) test. However, many people disagree with the validity of IQ tests...\" - Controversy suggests potentially false information.\n",
            "9. \"There is debate about if human intelligence is based on hereditary factors or if it is based on environmental factors. Hereditary intelligence is the theory that intelligence is fixed upon birth and does not grow.\" - The theories outlined here could potentially be disputed or false. \n",
            "10. \"Emotional intelligence is thought to be the ability to convey emotion to others in an understandable way as well as to read the emotions of others accurately.\" - This is a generalized statement that doesn't fully capture all aspects of emotional intelligence and could be misleading or false.\n",
            "11. \"Moral intelligence is the capacity to understand right from wrong and to behave based on the value that is believed to be right.\" - This is a generalized definition which might not encompass all views on moral intelligence, thus potentially false information.\n",
            "12. \"Concepts of \"book smarts\" and \"street smart\" are contrasting views based on the premise that some people have knowledge gained through academic study, but may lack the experience to sensibly apply that knowledge...\" - This generalization could potentially contain false information.\n",
            "________________UCLA\n",
            "1. \"Its academic roots were established in 1881 as a normal school then known as the southern branch of the California State Normal School which later evolved into San José State University.\"\n",
            "2. \"The branch was transferred to the University of California, becoming the Southern Branch of UC in 1919, making it the second-oldest of the ten-campus University of California system after the University of California, Berkeley.\"\n",
            "3. \"Three others are graduate-level professional health science schools: Medicine, Dentistry, and Public Health.\"\n",
            "4. \"They have won 121 NCAA team championships, second only to Stanford University's 128 team titles.\"\n",
            "5. \"410 Bruins have made Olympic teams, winning 270 Olympic medals: 136 gold, 71 silver and 63 bronze.\"\n",
            "6. \"In 1917, UC Regent Edward Augustus Dickson, the only regent representing the Southland at the time, and Ernest Carroll Moore, Director of the Normal School, began to lobby the State Legislature to enable the school to become the second University of California campus, after UC Berkeley.\"\n",
            "7. \"On May 23, 1919, the Southern Californians' efforts were rewarded when Governor William D. Stephens signed Assembly Bill 626 into law, which acquired the land and buildings and transformed the Los Angeles Normal School into the Southern Branch of the University of California.\"\n",
            "8. \"The Regents announced the new \"Beverly Site\" — just west of Beverly Hills — in 1925.\"\n",
            "9. \"On February 1, 1927, the Regents renamed the Southern Branch the University of California at Los Angeles.\"\n",
            "10. \"The campus in Westwood opened to students in 1929.\"\n",
            "11. \"The original four buildings were the College Library (now Powell Library), Royce Hall, the Physics-Biology Building (which became the Humanities Building and is now the Renee and David Kaplan Hall), and the Chemistry Building (now Haines Hall), arrayed around a quadrangular courtyard on the 400 acre (1.6 km2) campus.\"\n",
            "12. \"UCLA was permitted to award the master's degree in 1933, and the doctorate in 1936, against continued resistance from UC Berkeley.\"\n",
            "13. \"On June 1, 2016, two men were killed in a murder-suicide at an engineering building in the university.\"\n",
            "14. \"In February 2022, Matthew Harris, a former lecturer and postdoctoral fellow at UCLA, was arrested after allegedly making numerous threats of violence against students and faculty members of UCLA's Philosophy Department.\"\n",
            "15. \"In 2018, a student-led community coalition known as \"Westwood Forward\" successfully led an effort to break UCLA and Westwood Village away from the existing Westwood Neighborhood Council and form a new North Westwood Neighborhood Council, with over 2,000 out of 3,521 stakeholders voting in favor of the split.\"\n",
            "16. \"In 2022, UCLA signed an agreement to partner with the Tongva for the caretaking and landscaping of various areas of the campus.\"\n",
            "17. \"In 2014, a graduate student adviser and professor in the history department, Gabriel Piterberg, was accused of sexually assaulting two students.\"\n"
          ]
        }
      ]
    }
  ]
}